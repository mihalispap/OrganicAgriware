<oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
  <dc:title>Why observers should train clinical scoring</dc:title>
  <dc:creator>Dippel, Sabine</dc:creator>
  <dc:creator>Prunier, Armelle</dc:creator>
  <dc:creator>Leeb, Christine</dc:creator>
  <dc:subject>Animal husbandry</dc:subject>
  <dc:subject>Research methodology and philosophy</dc:subject>
  <dc:subject>Quality and evaluation of inputs</dc:subject>
  <dc:description>Epidemiological studies often involve clinical scoring of animals by several observers due to the high number of farms to be visited. Detailed written procedures and intensive observer training minimize variation between observers. This, however, is still not common in international cooperation. We present data on clinical assessment of sows from an EU project on organic pig health (COREPIG) to illustrate the consequences. &#xD;
     The clinical scoring system was based on procedures from the Welfare Quality Â® project and included measures regarding body condition (5-level scale), injuries (number of lesions &gt;3cm on shoulder, side and hindquarters), lameness (3-level scale), dirtiness (3-level scale) and skin problems (3-level scale). Nine observers from 6 EU countries trained clinical scoring during two days in two herds. Of the 9 observers, 4 had no or little, 2 had intermediate and 3 had extensive experience in working with pigs. Four observers each had little or intermediate experience in clinical scoring of sows and only 1 had extensive experience. Training comprised parameter discussions and joint scoring of animals. After training, each observer scored up to 30 pregnant sows per farm in 3 to 20 herds in six European countries as part of a larger epidemiological protocol. After completion of farm visits, observers scored up to 50 sows independently but at the same day and farm in order to assess inter-observer agreement. Parameters were collapsed into binary variables. We calculated Kendall's Coefficient of Concordance (W) across all observers and Prevalence Adjusted Bias Adjusted Kappas (PABAK) for observer pairs as measures of agreement. &#xD;
     Agreement across observers was not acceptable for skin problems and lameness (W &lt;0.41), and acceptable for dirtiness, obesity, and shoulder and hindquarters injuries (W between 0.41 and 0.60). Only for animal too thin and side injuries was W &gt;0.60 (N = 26 sows for skin problems, and 31 to 34 sows for other parameters). Pairwise agreement was not acceptable for skin problems and dirtiness (mean PABAK &lt;0.41) and acceptable for injuries shoulder and side (mean PABAK between 0.41 and 0.60). Agreement was good for hindquarter injuries and animal too thin (PABAK = 0.66 and 0.65, respectively), while obesity and lameness had mean PABAK of 0.84 and 0.95. Observer pairs scored 40 to 50 sows per parameter except for skin problems (36 to 49 sows). Results for lameness and obesity should be interpreted with care, as average prevalence across observers were only 3 and 8 %, respectively. Determination of whether a sow was too thin was the parameter with best agreement. The poor agreement for skin problems and dirtiness can be explained by misunderstandings regarding the parameter definition (e.g. inclusion of mud soiling). Extensive practical experience with pigs was of highest benefit for inter-observer agreement. Average PABAK was 0.70 (STD = 0.19, N = 24 scorings; 3 observer pairs, 8 parameters) for experienced observers but ranged between 0.49 and 0.56 (STD range 0.32 to 0.40) for all other combinations of experience level. The level of experience with clinical scoring of pigs did not have obvious positive effects. Average PABAK for all experience combinations ranged from 0.51 to 0.61 (STD range 0.32 to 0.40). By way of explanation, general experience with pigs helps to score an animal because observers will know a wider range of possible scenarios. By contrast, scores of observers who have already learned a scoring system will tend to be biased by their experience.&#xD;
     As a conclusion, our data emphasize the importance of intensive observer training before data collection and the need for inter-observer agreement tests before and after data collection.</dc:description>
  <dc:date>2010-03-24</dc:date>
  <dc:type>Conference paper, poster, etc.</dc:type>
  <dc:type>NonPeerReviewed</dc:type>
  <dc:format>application/pdf</dc:format>
  <dc:identifier>http://orgprints.org/18415/1/18415.pdf</dc:identifier>
  <dc:identifier>Dippel, Sabine; Prunier, Armelle and Leeb, Christine    (2010) Why observers should train clinical scoring.       Poster at:   2010 SVEPM Annual Conference, Nantes, France, 26.-26.03.2010.</dc:identifier>
  <dc:relation>http://orgprints.org/18415/</dc:relation>
</oai_dc:dc>